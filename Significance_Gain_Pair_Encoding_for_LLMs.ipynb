{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meetra21/LLM_24/blob/main/Significance_Gain_Pair_Encoding_for_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 — Install\n",
        "!pip -q install datasets"
      ],
      "metadata": {
        "id": "4qDKJlT3-PLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 — Imports + seed + device\n",
        "import re, math, time, random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "gbt4opOn-PLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 — Load WikiText-103 + normalize\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "\n",
        "train_text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
        "val_text   = \"\\n\".join(ds[\"validation\"][\"text\"])\n",
        "test_text  = \"\\n\".join(ds[\"test\"][\"text\"])\n",
        "\n",
        "def normalize_space(s: str) -> str:\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "train_text = normalize_space(train_text)\n",
        "val_text   = normalize_space(val_text)\n",
        "test_text  = normalize_space(test_text)\n",
        "\n",
        "train_text_small = train_text[:200_000]\n",
        "val_text_small   = val_text[:200_000]\n",
        "test_text_small  = test_text[:200_000]\n",
        "\n",
        "len(train_text_small), len(val_text_small), len(test_text_small)"
      ],
      "metadata": {
        "id": "bHoZ1Le6-PLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 — pretokenizer (English word-ish tokens)\n",
        "def pretokenize_english(s: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Pretokenize into word-ish units to speed up BPE training.\n",
        "    Keeps leading space attached: ' the', ' verdict'\n",
        "    Splits punctuation and newlines into separate tokens.\n",
        "    \"\"\"\n",
        "    # order matters: handle paragraph breaks/newlines first\n",
        "    pattern = r\"\\n\\n|\\n| ?[A-Za-z]+| ?\\d+| ?[^A-Za-z0-9\\s]+| +\"\n",
        "    return re.findall(pattern, s)\n",
        "\n",
        "\n",
        "def pretokenize_chars(s: str) -> List[str]:\n",
        "    # Character-level base tokens\n",
        "    return list(s)"
      ],
      "metadata": {
        "id": "GiPo3LCR-PLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 — NEW IDEA function (paper contribution)\n",
        "def significance_gain_score(\n",
        "    cxy: int,\n",
        "    cx: int,\n",
        "    cy: int,\n",
        "    N: int,\n",
        "    eps: float = 1e-9,\n",
        "    alpha: float = 0.25,\n",
        "    use_gain: bool = True,\n",
        "    lambda_rare: float = 0.0,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    NEW IDEA: Significance-Gain score for BPE merges.\n",
        "\n",
        "    Null model: independence\n",
        "      E(xy) = cx * cy / N\n",
        "\n",
        "    Surprise:\n",
        "      z = (cxy - E) / sqrt(E + eps)\n",
        "\n",
        "    Tempering:\n",
        "      z <- z * (cxy ** alpha)\n",
        "\n",
        "    Compression-aware gain:\n",
        "      gain ≈ cxy\n",
        "\n",
        "    Optional rare penalty:\n",
        "      lambda_rare / sqrt(cxy)\n",
        "    \"\"\"\n",
        "    N = max(N, 1)\n",
        "    expected = (cx * cy) / N\n",
        "    z = (cxy - expected) / math.sqrt(expected + eps)\n",
        "\n",
        "    if alpha and alpha > 0:\n",
        "        z = z * (cxy ** alpha)\n",
        "\n",
        "    gain = float(cxy) if use_gain else 1.0\n",
        "    pen = (lambda_rare / math.sqrt(cxy + eps)) if (lambda_rare and lambda_rare > 0) else 0.0\n",
        "    return gain * z - pen"
      ],
      "metadata": {
        "id": "AMi5vBC2-PLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 — Tokenizer config + tokenizer class\n",
        "@dataclass\n",
        "class BPEConfig:\n",
        "    vocab_size: int = 2000\n",
        "    min_pair_count: int = 5\n",
        "    eps: float = 1e-9\n",
        "    unk_token: str = \"<unk>\"\n",
        "\n",
        "    # New-idea knobs\n",
        "    alpha_count: float = 0.25\n",
        "    use_gain: bool = True\n",
        "    lambda_rare: float = 0.0\n",
        "\n",
        "\n",
        "class SimpleBPE:\n",
        "    def __init__(self, config: BPEConfig, mode: str = \"freq\"):\n",
        "        assert mode in (\"freq\", \"sigz\")\n",
        "        self.cfg = config\n",
        "        self.mode = mode\n",
        "        self.merges: List[Tuple[Tuple[str, str], str]] = []\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.id_to_tok: List[str] = []\n",
        "\n",
        "    def _get_pair_counts(self, seq: List[str]) -> Counter:\n",
        "        return Counter(zip(seq, seq[1:]))\n",
        "\n",
        "    def _get_symbol_counts(self, seq: List[str]) -> Counter:\n",
        "        return Counter(seq)\n",
        "\n",
        "    def _apply_merge_once(self, seq: List[str], pair: Tuple[str, str], merged: str) -> List[str]:\n",
        "        a, b = pair\n",
        "        out = []\n",
        "        i = 0\n",
        "        L = len(seq)\n",
        "        while i < L:\n",
        "            if i < L - 1 and seq[i] == a and seq[i + 1] == b:\n",
        "                out.append(merged)\n",
        "                i += 2\n",
        "            else:\n",
        "                out.append(seq[i])\n",
        "                i += 1\n",
        "        return out\n",
        "\n",
        "    def _choose_best_pair(self, seq: List[str]) -> Optional[Tuple[str, str]]:\n",
        "        pair_counts = self._get_pair_counts(seq)\n",
        "        candidates = [(p, c) for p, c in pair_counts.items() if c >= self.cfg.min_pair_count]\n",
        "        if not candidates:\n",
        "            return None\n",
        "\n",
        "        # Baseline BPE = max count\n",
        "        if self.mode == \"freq\":\n",
        "            best_pair, _ = max(candidates, key=lambda x: x[1])\n",
        "            return best_pair\n",
        "\n",
        "        # NEW IDEA = maximize significance_gain_score\n",
        "        sym_counts = self._get_symbol_counts(seq)\n",
        "        N = max(len(seq) - 1, 1)\n",
        "\n",
        "        best_pair = None\n",
        "        best_score = -1e30\n",
        "        for (a, b), cxy in candidates:\n",
        "            cx = sym_counts[a]\n",
        "            cy = sym_counts[b]\n",
        "            score = significance_gain_score(\n",
        "                cxy=cxy, cx=cx, cy=cy, N=N,\n",
        "                eps=self.cfg.eps,\n",
        "                alpha=self.cfg.alpha_count,\n",
        "                use_gain=self.cfg.use_gain,\n",
        "                lambda_rare=self.cfg.lambda_rare\n",
        "            )\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_pair = (a, b)\n",
        "        return best_pair\n",
        "\n",
        "    def train(self, text: str, progress_every: int = 200):\n",
        "\n",
        "        seq = pretokenize_chars(text)\n",
        "\n",
        "        # base vocab from TRAIN pre-tokens + <unk>\n",
        "        base_vocab = sorted(set(seq))\n",
        "        if self.cfg.unk_token not in base_vocab:\n",
        "            base_vocab.append(self.cfg.unk_token)\n",
        "\n",
        "        self.merges = []\n",
        "\n",
        "        def current_vocab_size(s):\n",
        "            return len(set(s))\n",
        "\n",
        "\n",
        "        #target = max(self.cfg.vocab_size, current_vocab_size(seq))\n",
        "        target = self.cfg.vocab_size\n",
        "        t0 = time.time()\n",
        "        it = 0\n",
        "\n",
        "        while current_vocab_size(seq) < target:\n",
        "            best_pair = self._choose_best_pair(seq)\n",
        "            if best_pair is None:\n",
        "                break\n",
        "\n",
        "            merged = best_pair[0] + best_pair[1]\n",
        "            self.merges.append((best_pair, merged))\n",
        "            seq = self._apply_merge_once(seq, best_pair, merged)\n",
        "\n",
        "            it += 1\n",
        "            if it % progress_every == 0:\n",
        "                print(f\"[{self.mode}] merges={it} vocab={current_vocab_size(seq)} elapsed={time.time()-t0:.1f}s\")\n",
        "\n",
        "            if it > 20000:\n",
        "                print(\"[warn] stopping at 20000 merges (safety).\")\n",
        "                break\n",
        "\n",
        "        # Final vocab\n",
        "        tokens = set(base_vocab)\n",
        "        for (_, _), merged in self.merges:\n",
        "            tokens.add(merged)\n",
        "        tokens.add(self.cfg.unk_token)\n",
        "\n",
        "        self.id_to_tok = sorted(tokens)\n",
        "        self.vocab = {t: i for i, t in enumerate(self.id_to_tok)}\n",
        "        print(f\"[{self.mode}] training done: merges={len(self.merges)} final_vocab={len(self.vocab)}\")\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        UNK = self.cfg.unk_token\n",
        "        unk_id = self.vocab[UNK]\n",
        "\n",
        "        seq = pretokenize_chars(text)\n",
        "\n",
        "        # Map unseen base tokens to UNK\n",
        "        seq = [t if t in self.vocab else UNK for t in seq]\n",
        "\n",
        "        for (a, b), merged in self.merges:\n",
        "            seq = self._apply_merge_once(seq, (a, b), merged)\n",
        "\n",
        "        return [self.vocab.get(t, unk_id) for t in seq]\n",
        "\n",
        "    def stats(self, encoded: List[int], raw_text: str) -> Dict[str, float]:\n",
        "        toks = len(encoded)\n",
        "        chars = max(len(raw_text), 1)\n",
        "        used = len(set(encoded))\n",
        "        util = used / max(len(self.vocab), 1)\n",
        "        return {\n",
        "            \"tokens_per_char\": toks / chars,\n",
        "            \"vocab_used_frac\": util,\n",
        "            \"num_tokens\": toks,\n",
        "            \"num_chars\": chars,\n",
        "        }"
      ],
      "metadata": {
        "id": "yW8KajIF-PLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 — Train tokenizers + encode + show stats\n",
        "def show_stats(name, tok, tr, va, te, train_text, val_text, test_text):\n",
        "    st_tr = tok.stats(tr, train_text)\n",
        "    st_va = tok.stats(va, val_text)\n",
        "    st_te = tok.stats(te, test_text)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"  train tpc={st_tr['tokens_per_char']:.4f}  vocab_used={st_tr['vocab_used_frac']:.3f}\")\n",
        "    print(f\"  val   tpc={st_va['tokens_per_char']:.4f}  vocab_used={st_va['vocab_used_frac']:.3f}\")\n",
        "    print(f\"  test  tpc={st_te['tokens_per_char']:.4f}  vocab_used={st_te['vocab_used_frac']:.3f}\")\n",
        "\n",
        "vocab_size = 600\n",
        "tok_freq = SimpleBPE(BPEConfig(vocab_size=vocab_size, min_pair_count=5, alpha_count=0.0), mode=\"freq\")\n",
        "tok_sigz = SimpleBPE(BPEConfig(vocab_size=vocab_size, min_pair_count=5, alpha_count=0.25, use_gain=True), mode=\"sigz\")\n",
        "\n",
        "print(\"Training standard BPE...\")\n",
        "tok_freq.train(train_text_small, progress_every=200)\n",
        "\n",
        "print(\"\\nTraining significance-gain BPE...\")\n",
        "tok_sigz.train(train_text_small, progress_every=200)\n",
        "\n",
        "tr_f = tok_freq.encode(train_text_small)\n",
        "va_f = tok_freq.encode(val_text_small)\n",
        "te_f = tok_freq.encode(test_text_small)\n",
        "\n",
        "tr_s = tok_sigz.encode(train_text_small)\n",
        "va_s = tok_sigz.encode(val_text_small)\n",
        "te_s = tok_sigz.encode(test_text_small)\n",
        "\n",
        "show_stats(\"BPE freq\", tok_freq, tr_f, va_f, te_f, train_text_small, val_text_small, test_text_small)\n",
        "show_stats(\"BPE sigz (gain*z)\", tok_sigz, tr_s, va_s, te_s, train_text_small, val_text_small, test_text_small)"
      ],
      "metadata": {
        "id": "nWGFC1dh-PLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8 — Dataset blocks for LM\n",
        "class LMBlocks(Dataset):\n",
        "    def __init__(self, ids: List[int], block_size: int = 256, stride: int = 256):\n",
        "        self.ids = torch.tensor(ids, dtype=torch.long)\n",
        "        self.block = block_size\n",
        "        self.starts = list(range(0, max(len(self.ids) - block_size - 1, 0), stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.starts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.starts[idx]\n",
        "        x = self.ids[s:s+self.block]\n",
        "        y = self.ids[s+1:s+self.block+1]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "-pQxvAxm-PLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9 — Tiny GPT model\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.nh = n_heads\n",
        "        self.dk = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.nh, self.dk).transpose(1, 2)\n",
        "        k = k.view(B, T, self.nh, self.dk).transpose(1, 2)\n",
        "        v = v.view(B, T, self.nh, self.dk).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.dk)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "        att = att.masked_fill(mask, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.drop(att)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float, mlp_mult: int = 4):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, mlp_mult * d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_mult * d_model, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size: int, block_size: int, d_model=192, n_heads=4, n_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        pos = torch.arange(0, T, device=idx.device)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "l3beQjOS-PLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 10 — Train/eval helpers\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    correct = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction=\"sum\")\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += y.numel()\n",
        "\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        correct += (pred == y).sum().item()\n",
        "\n",
        "    avg_nll = total_loss / max(total_tokens, 1)\n",
        "    ppl = math.exp(min(avg_nll, 20))\n",
        "    acc = correct / max(total_tokens, 1)\n",
        "    return avg_nll, ppl, acc\n",
        "\n",
        "def train_lm(train_ids, val_ids, vocab_size, block_size=256, batch_size=64, epochs=3, lr=3e-4):\n",
        "    train_ds = LMBlocks(train_ids, block_size=block_size, stride=block_size)\n",
        "    val_ds   = LMBlocks(val_ids, block_size=block_size, stride=block_size)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = TinyGPT(vocab_size=vocab_size, block_size=block_size).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "        running = 0.0\n",
        "        tokens = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            running += loss.item() * y.numel()\n",
        "            tokens += y.numel()\n",
        "\n",
        "        train_nll = running / max(tokens, 1)\n",
        "        val_nll, val_ppl, val_acc = evaluate(model, val_loader, device)\n",
        "        print(f\"epoch {ep:02d} | train_nll={train_nll:.4f} | val_nll={val_nll:.4f} | val_ppl={val_ppl:.2f} | val_acc={val_acc*100:.2f}% | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rcngO4FZ-PLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CELL 11— experiment runner:\n",
        "# multi-seed, vocab grid, ablations, BPC vs TPC plots, top merges\n",
        "# =========================\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import replace\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "LN2 = math.log(2.0)\n",
        "\n",
        "def nll_char(nll_tok: float, tokens_per_char: float) -> float:\n",
        "    return nll_tok * tokens_per_char\n",
        "\n",
        "def bpc(nll_tok: float, tokens_per_char: float) -> float:\n",
        "    return nll_char(nll_tok, tokens_per_char) / LN2\n",
        "\n",
        "def mean_std(xs):\n",
        "    xs = np.array(xs, dtype=float)\n",
        "    return float(xs.mean()), float(xs.std(ddof=1)) if len(xs) > 1 else 0.0\n",
        "\n",
        "def print_top_merges(tok, k=30, title=\"Top merges\"):\n",
        "    print(f\"\\n--- {title} (first {k} merges in training order) ---\")\n",
        "    for i, ((a,b), ab) in enumerate(tok.merges[:k], 1):\n",
        "        # show whitespace clearly\n",
        "        aa = a.replace(\"\\n\", \"\\\\n\")\n",
        "        bb = b.replace(\"\\n\", \"\\\\n\")\n",
        "        abab = ab.replace(\"\\n\", \"\\\\n\")\n",
        "        print(f\"{i:02d}: ({aa!r}, {bb!r}) -> {abab!r}\")\n",
        "\n",
        "def build_tokenizer(mode: str, cfg: BPEConfig):\n",
        "    return SimpleBPE(cfg, mode=mode)\n",
        "\n",
        "def train_tokenizer_and_encode(cfg: BPEConfig, mode: str,\n",
        "                              train_text, val_text, test_text):\n",
        "    \"\"\"\n",
        "    Trains tokenizer ONCE, encodes all splits ONCE.\n",
        "    Returns tok, encoded ids, tpc stats.\n",
        "    \"\"\"\n",
        "    tok = build_tokenizer(mode, cfg)\n",
        "    tok.train(train_text)\n",
        "\n",
        "    tr_ids = tok.encode(train_text)\n",
        "    va_ids = tok.encode(val_text)\n",
        "    te_ids = tok.encode(test_text)\n",
        "\n",
        "    st_tr = tok.stats(tr_ids, train_text)\n",
        "    st_va = tok.stats(va_ids, val_text)\n",
        "    st_te = tok.stats(te_ids, test_text)\n",
        "\n",
        "    out = {\n",
        "        \"tok\": tok,\n",
        "        \"tr\": tr_ids,\n",
        "        \"va\": va_ids,\n",
        "        \"te\": te_ids,\n",
        "        \"tpc_tr\": st_tr[\"tokens_per_char\"],\n",
        "        \"tpc_va\": st_va[\"tokens_per_char\"],\n",
        "        \"tpc_te\": st_te[\"tokens_per_char\"],\n",
        "        \"vocab_size\": len(tok.vocab),\n",
        "        \"merges\": len(tok.merges),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "def run_lm_multi_seed(tr_ids, va_ids, te_ids, vocab_size,\n",
        "                      seeds=(1,2,3,4,5), block_size=256, epochs=3, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train/eval LM across multiple seeds. Returns lists of metrics per seed.\n",
        "    Uses existing train_lm(...) and evaluate(...).\n",
        "    \"\"\"\n",
        "    val_ppl_list, test_ppl_list = [], []\n",
        "    val_nll_list, test_nll_list = [], []\n",
        "    val_acc_list, test_acc_list = [], []\n",
        "\n",
        "    for s in seeds:\n",
        "        set_seed(s)\n",
        "\n",
        "        model = train_lm(tr_ids, va_ids, vocab_size=vocab_size,\n",
        "                         block_size=block_size, epochs=epochs)\n",
        "\n",
        "        val_loader = DataLoader(LMBlocks(va_ids, block_size=block_size, stride=block_size),\n",
        "                                batch_size=batch_size, shuffle=False)\n",
        "        te_loader  = DataLoader(LMBlocks(te_ids, block_size=block_size, stride=block_size),\n",
        "                                batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        val_nll, val_ppl, val_acc = evaluate(model, val_loader, device)\n",
        "        te_nll,  te_ppl,  te_acc  = evaluate(model, te_loader, device)\n",
        "\n",
        "        val_ppl_list.append(val_ppl); test_ppl_list.append(te_ppl)\n",
        "        val_nll_list.append(val_nll); test_nll_list.append(te_nll)\n",
        "        val_acc_list.append(val_acc); test_acc_list.append(te_acc)\n",
        "\n",
        "        hard_cleanup(model)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"val_ppl\": val_ppl_list, \"test_ppl\": test_ppl_list,\n",
        "        \"val_nll\": val_nll_list, \"test_nll\": test_nll_list,\n",
        "        \"val_acc\": val_acc_list, \"test_acc\": test_acc_list,\n",
        "    }\n",
        "\n",
        "def summarize_run(name, lm_metrics, tpc_va, tpc_te):\n",
        "    \"\"\"\n",
        "    Converts LM metrics into mean±std for PPL and BPC (tokenizer-invariant).\n",
        "    \"\"\"\n",
        "    val_ppl_m, val_ppl_s = mean_std(lm_metrics[\"val_ppl\"])\n",
        "    test_ppl_m, test_ppl_s = mean_std(lm_metrics[\"test_ppl\"])\n",
        "\n",
        "    # Convert NLL/token -> BPC using tpc\n",
        "    val_bpc_list  = [bpc(nll, tpc_va) for nll in lm_metrics[\"val_nll\"]]\n",
        "    test_bpc_list = [bpc(nll, tpc_te) for nll in lm_metrics[\"test_nll\"]]\n",
        "    val_bpc_m, val_bpc_s = mean_std(val_bpc_list)\n",
        "    test_bpc_m, test_bpc_s = mean_std(test_bpc_list)\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"  Val  PPL: {val_ppl_m:.2f} ± {val_ppl_s:.2f}\")\n",
        "    print(f\"  Test PPL: {test_ppl_m:.2f} ± {test_ppl_s:.2f}\")\n",
        "    print(f\"  Val  BPC: {val_bpc_m:.4f} ± {val_bpc_s:.4f}   (tpc={tpc_va:.4f})\")\n",
        "    print(f\"  Test BPC: {test_bpc_m:.4f} ± {test_bpc_s:.4f}   (tpc={tpc_te:.4f})\")\n",
        "\n",
        "    return {\n",
        "        \"val_ppl_mean\": val_ppl_m, \"val_ppl_std\": val_ppl_s,\n",
        "        \"test_ppl_mean\": test_ppl_m, \"test_ppl_std\": test_ppl_s,\n",
        "        \"val_bpc_mean\": val_bpc_m, \"val_bpc_std\": val_bpc_s,\n",
        "        \"test_bpc_mean\": test_bpc_m, \"test_bpc_std\": test_bpc_s,\n",
        "        \"tpc_va\": tpc_va, \"tpc_te\": tpc_te,\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG: choose seeds, vocab grid, epochs\n",
        "# -------------------------\n",
        "SEEDS = [1,2,3,4,5]        # bump to 10 later\n",
        "EPOCHS = 3                 # keep small while iterating\n",
        "BLOCK_SIZE = 256\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Grid of vocab sizes (for BPC vs TPC curves)\n",
        "VOCAB_GRID = [300, 400, 600, 800, 1200]\n",
        "\n",
        "# Base fairness knobs (keep same across all tokenizers)\n",
        "MIN_PAIR = 5\n",
        "LAMBDA_RARE = 0.0\n",
        "\n",
        "# -------------------------\n",
        "# 1) ABLATIONS at ONE vocab size\n",
        "# -------------------------\n",
        "V0 = 400\n",
        "\n",
        "ablations = [\n",
        "    (\"Freq-BPE\",               (\"freq\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=0.0, use_gain=False, lambda_rare=LAMBDA_RARE))),\n",
        "    (\"SigGain alpha=0.00\",     (\"sigz\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=0.0, use_gain=True,  lambda_rare=LAMBDA_RARE))),\n",
        "    (\"SigGain alpha=0.25\",     (\"sigz\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=0.25,use_gain=True,  lambda_rare=LAMBDA_RARE))),\n",
        "    (\"SigGain alpha=0.50\",     (\"sigz\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=0.5, use_gain=True,  lambda_rare=LAMBDA_RARE))),\n",
        "    (\"SigGain alpha=1.00\",     (\"sigz\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=1.0, use_gain=True,  lambda_rare=LAMBDA_RARE))),\n",
        "    (\"SigOnly (no gain)\",      (\"sigz\", BPEConfig(vocab_size=V0, min_pair_count=MIN_PAIR, alpha_count=0.25,use_gain=False, lambda_rare=LAMBDA_RARE))),\n",
        "]\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"ABLATIONS (multi-seed results)\")\n",
        "print(\"==============================\")\n",
        "\n",
        "ablation_summaries = []\n",
        "for name, (mode, cfg) in ablations:\n",
        "    pack = train_tokenizer_and_encode(cfg, mode, train_text_small, val_text_small, test_text_small)\n",
        "\n",
        "    print(f\"\\nTokenizer: {name} | merges={pack['merges']} | vocab={pack['vocab_size']} | \"\n",
        "          f\"tpc(val)={pack['tpc_va']:.4f} tpc(test)={pack['tpc_te']:.4f}\")\n",
        "\n",
        "    # Qualitative: show merges for the main baseline + main siggain\n",
        "    if name in (\"Freq-BPE\", \"SigGain alpha=0.25\"):\n",
        "        print_top_merges(pack[\"tok\"], k=25, title=f\"{name}\")\n",
        "\n",
        "    lm = run_lm_multi_seed(pack[\"tr\"], pack[\"va\"], pack[\"te\"],\n",
        "                           vocab_size=pack[\"vocab_size\"],\n",
        "                           seeds=SEEDS, block_size=BLOCK_SIZE,\n",
        "                           epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    summ = summarize_run(name, lm, pack[\"tpc_va\"], pack[\"tpc_te\"])\n",
        "    summ[\"name\"] = name\n",
        "    summ[\"mode\"] = mode\n",
        "    summ[\"vocab_target\"] = cfg.vocab_size\n",
        "    summ[\"merges\"] = pack[\"merges\"]\n",
        "    ablation_summaries.append(summ)\n",
        "\n",
        "# -------------------------\n",
        "# 2) BPC vs TPC curves over a vocab grid\n",
        "# -------------------------\n",
        "print(\"\\n==============================\")\n",
        "print(\"VOCAB GRID (BPC vs TPC curves)\")\n",
        "print(\"==============================\")\n",
        "\n",
        "grid_rows = []\n",
        "\n",
        "for V in VOCAB_GRID:\n",
        "    # Frequency baseline\n",
        "    cfg_f = BPEConfig(vocab_size=V, min_pair_count=MIN_PAIR, alpha_count=0.0, use_gain=False, lambda_rare=LAMBDA_RARE)\n",
        "    pack_f = train_tokenizer_and_encode(cfg_f, \"freq\", train_text_small, val_text_small, test_text_small)\n",
        "    lm_f = run_lm_multi_seed(pack_f[\"tr\"], pack_f[\"va\"], pack_f[\"te\"],\n",
        "                             vocab_size=pack_f[\"vocab_size\"],\n",
        "                             seeds=SEEDS, block_size=BLOCK_SIZE,\n",
        "                             epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "    summ_f = summarize_run(f\"Freq-BPE (V={V})\", lm_f, pack_f[\"tpc_va\"], pack_f[\"tpc_te\"])\n",
        "    summ_f.update({\"tokenizer\":\"freq\", \"V\":V})\n",
        "\n",
        "    # SigGain main setting\n",
        "    cfg_s = BPEConfig(vocab_size=V, min_pair_count=MIN_PAIR, alpha_count=0.25, use_gain=True, lambda_rare=LAMBDA_RARE)\n",
        "    pack_s = train_tokenizer_and_encode(cfg_s, \"sigz\", train_text_small, val_text_small, test_text_small)\n",
        "    lm_s = run_lm_multi_seed(pack_s[\"tr\"], pack_s[\"va\"], pack_s[\"te\"],\n",
        "                             vocab_size=pack_s[\"vocab_size\"],\n",
        "                             seeds=SEEDS, block_size=BLOCK_SIZE,\n",
        "                             epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "    summ_s = summarize_run(f\"SigGain (V={V})\", lm_s, pack_s[\"tpc_va\"], pack_s[\"tpc_te\"])\n",
        "    summ_s.update({\"tokenizer\":\"siggain\", \"V\":V})\n",
        "\n",
        "    grid_rows.append(summ_f)\n",
        "    grid_rows.append(summ_s)\n",
        "\n",
        "# Plot BPC vs TPC (validation)\n",
        "freq_pts = [(r[\"tpc_va\"], r[\"val_bpc_mean\"]) for r in grid_rows if r[\"tokenizer\"]==\"freq\"]\n",
        "sig_pts  = [(r[\"tpc_va\"], r[\"val_bpc_mean\"]) for r in grid_rows if r[\"tokenizer\"]==\"siggain\"]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([x for x,_ in freq_pts], [y for _,y in freq_pts], marker=\"o\", label=\"Freq-BPE\")\n",
        "plt.plot([x for x,_ in sig_pts],  [y for _,y in sig_pts],  marker=\"o\", label=\"SigGain-BPE (alpha=0.25)\")\n",
        "plt.xlabel(\"tokens per character (val)\")\n",
        "plt.ylabel(\"bits per character (val)\")\n",
        "plt.title(\"BPC vs TPC (validation)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 3) Match-compression comparison (closest TPC)\n",
        "# -------------------------\n",
        "print(\"\\n=========================================\")\n",
        "print(\"MATCHED COMPRESSION (closest val TPC)\")\n",
        "print(\"=========================================\")\n",
        "\n",
        "freq_grid = [r for r in grid_rows if r[\"tokenizer\"]==\"freq\"]\n",
        "sig_grid  = [r for r in grid_rows if r[\"tokenizer\"]==\"siggain\"]\n",
        "\n",
        "def closest_by_tpc(target_tpc, candidates):\n",
        "    return min(candidates, key=lambda r: abs(r[\"tpc_va\"] - target_tpc))\n",
        "\n",
        "for r_sig in sig_grid:\n",
        "    r_freq = closest_by_tpc(r_sig[\"tpc_va\"], freq_grid)\n",
        "    print(f\"\\nSigGain V={r_sig['V']:>4} (tpc={r_sig['tpc_va']:.4f}, val_bpc={r_sig['val_bpc_mean']:.4f})\"\n",
        "          f\"  vs  Freq V={r_freq['V']:>4} (tpc={r_freq['tpc_va']:.4f}, val_bpc={r_freq['val_bpc_mean']:.4f})\"\n",
        "          f\"  | ΔBPC={r_freq['val_bpc_mean']-r_sig['val_bpc_mean']:+.4f}\")"
      ],
      "metadata": {
        "id": "94Iqq--6-PLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 — Train both LMs + final comparison (also reports tokenizer-invariant BPC)\n",
        "import math\n",
        "\n",
        "block_size = 256\n",
        "epochs = 8\n",
        "\n",
        "# --- Helper: tokenizer-invariant metrics ---\n",
        "LN2 = math.log(2.0)\n",
        "\n",
        "def nll_char(nll_tok: float, tokens_per_char: float) -> float:\n",
        "    \"\"\"Convert NLL per token (nats/token) to NLL per character (nats/char).\"\"\"\n",
        "    return nll_tok * tokens_per_char\n",
        "\n",
        "def bpc(nll_tok: float, tokens_per_char: float) -> float:\n",
        "    \"\"\"Bits per character (BPC) = (nats/char) / ln(2).\"\"\"\n",
        "    return nll_char(nll_tok, tokens_per_char) / LN2\n",
        "\n",
        "def pct_improve(old, new):\n",
        "    return (old - new) / old * 100.0\n",
        "\n",
        "print(\"\\n=== Train LM with standard BPE ===\")\n",
        "model_f = train_lm(tr_f, va_f, vocab_size=len(tok_freq.vocab), block_size=block_size, epochs=epochs)\n",
        "\n",
        "print(\"\\n=== Train LM with significance-gain BPE ===\")\n",
        "model_s = train_lm(tr_s, va_s, vocab_size=len(tok_sigz.vocab), block_size=block_size, epochs=epochs)\n",
        "\n",
        "# --- Dataloaders ---\n",
        "val_loader_f = DataLoader(LMBlocks(va_f, block_size=block_size, stride=block_size), batch_size=64, shuffle=False)\n",
        "val_loader_s = DataLoader(LMBlocks(va_s, block_size=block_size, stride=block_size), batch_size=64, shuffle=False)\n",
        "te_loader_f  = DataLoader(LMBlocks(te_f, block_size=block_size, stride=block_size), batch_size=64, shuffle=False)\n",
        "te_loader_s  = DataLoader(LMBlocks(te_s, block_size=block_size, stride=block_size), batch_size=64, shuffle=False)\n",
        "\n",
        "# --- Evaluate (returns NLL in nats/token, PPL, token-acc) ---\n",
        "val_nll_f, val_ppl_f, val_acc_f = evaluate(model_f, val_loader_f, device)\n",
        "val_nll_s, val_ppl_s, val_acc_s = evaluate(model_s, val_loader_s, device)\n",
        "te_nll_f,  te_ppl_f,  te_acc_f  = evaluate(model_f, te_loader_f, device)\n",
        "te_nll_s,  te_ppl_s,  te_acc_s  = evaluate(model_s, te_loader_s, device)\n",
        "\n",
        "# --- Get tokens-per-character for each tokenizer on val/test (from existing tok.stats) ---\n",
        "st_va_f = tok_freq.stats(va_f, val_text_small)\n",
        "st_te_f = tok_freq.stats(te_f, test_text_small)\n",
        "st_va_s = tok_sigz.stats(va_s, val_text_small)\n",
        "st_te_s = tok_sigz.stats(te_s, test_text_small)\n",
        "\n",
        "val_tpc_f, test_tpc_f = st_va_f[\"tokens_per_char\"], st_te_f[\"tokens_per_char\"]\n",
        "val_tpc_s, test_tpc_s = st_va_s[\"tokens_per_char\"], st_te_s[\"tokens_per_char\"]\n",
        "\n",
        "# --- Tokenizer-invariant metrics: NLL/char + BPC ---\n",
        "val_nllc_f  = nll_char(val_nll_f, val_tpc_f)\n",
        "test_nllc_f = nll_char(te_nll_f,  test_tpc_f)\n",
        "val_bpc_f   = bpc(val_nll_f, val_tpc_f)\n",
        "test_bpc_f  = bpc(te_nll_f,  test_tpc_f)\n",
        "\n",
        "val_nllc_s  = nll_char(val_nll_s, val_tpc_s)\n",
        "test_nllc_s = nll_char(te_nll_s,  test_tpc_s)\n",
        "val_bpc_s   = bpc(val_nll_s, val_tpc_s)\n",
        "test_bpc_s  = bpc(te_nll_s,  test_tpc_s)\n",
        "\n",
        "print(\"\\n====================\")\n",
        "print(\"FINAL COMPARISON\")\n",
        "print(\"====================\")\n",
        "print(f\"Standard BPE     | val_ppl={val_ppl_f:.2f} | val_acc={val_acc_f*100:.2f}% | test_ppl={te_ppl_f:.2f} | test_acc={te_acc_f*100:.2f}%\")\n",
        "print(f\"Significance-BPE | val_ppl={val_ppl_s:.2f} | val_acc={val_acc_s*100:.2f}% | test_ppl={te_ppl_s:.2f} | test_acc={te_acc_s*100:.2f}%\")\n",
        "\n",
        "print(\"\\nTokenizer stats (tokens per char)\")\n",
        "print(f\"Standard BPE     | val_tpc={val_tpc_f:.4f} | test_tpc={test_tpc_f:.4f}\")\n",
        "print(f\"Significance-BPE | val_tpc={val_tpc_s:.4f} | test_tpc={test_tpc_s:.4f}\")\n",
        "\n",
        "print(\"\\nTokenizer-invariant metrics\")\n",
        "print(f\"Standard BPE     | val_nll/char={val_nllc_f:.6f} | test_nll/char={test_nllc_f:.6f} | val_bpc={val_bpc_f:.4f} | test_bpc={test_bpc_f:.4f}\")\n",
        "print(f\"Significance-BPE | val_nll/char={val_nllc_s:.6f} | test_nll/char={test_nllc_s:.6f} | val_bpc={val_bpc_s:.4f} | test_bpc={test_bpc_s:.4f}\")\n",
        "\n",
        "print(\"\\nImprovements (lower is better)\")\n",
        "print(f\"Val  PPL improvement (%): {pct_improve(val_ppl_f, val_ppl_s):.2f}\")\n",
        "print(f\"Test PPL improvement (%): {pct_improve(te_ppl_f,  te_ppl_s):.2f}\")\n",
        "print(f\"Val  BPC improvement (%): {pct_improve(val_bpc_f, val_bpc_s):.2f}\")\n",
        "print(f\"Test BPC improvement (%): {pct_improve(test_bpc_f, test_bpc_s):.2f}\")"
      ],
      "metadata": {
        "id": "0KNoBDa3-PLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}